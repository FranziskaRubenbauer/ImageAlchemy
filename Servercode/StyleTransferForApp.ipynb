{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4749f23d-9350-40c0-bbfa-4c9a0924999f",
   "metadata": {
    "executionInfo": {
     "elapsed": 3335,
     "status": "ok",
     "timestamp": 1700749620102,
     "user": {
      "displayName": "Franziska Rubenbauer",
      "userId": "01843698082611866585"
     },
     "user_tz": -60
    },
    "id": "8c2501c8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "import torch.optim as optimizers\n",
    "from PIL import Image\n",
    "import numpy\n",
    "import matplotlib.pyplot as pyplot\n",
    "from IPython import display\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf6018a",
   "metadata": {
    "executionInfo": {
     "elapsed": 4836,
     "status": "ok",
     "timestamp": 1700749624929,
     "user": {
      "displayName": "Franziska Rubenbauer",
      "userId": "01843698082611866585"
     },
     "user_tz": -60
    },
    "id": "0cf6018a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#VGG-19 besteht aus zwei Teilen: 1. Convolutions+Pooling 2. Classifieres\n",
    "# .features beschrenkt das Model auf den 1. Teil (also nur die Convolutions)\n",
    "vgg_model = models.vgg19(weights='DEFAULT').features\n",
    "\n",
    "#freezing parameters -> nur target soll verändert werden\n",
    "for parameter in vgg_model.parameters():\n",
    "    parameter.requires_grad_(False) #only optimize target image, not the other parameters\n",
    "\n",
    "torch_device = torch.device(\"cuda:5\" if torch.cuda.is_available() else \"cpu\") #use gpu if possible\n",
    "\n",
    "vgg_model.to(torch_device)\n",
    "\n",
    "def load_image(path_to_image, shape=None):\n",
    "    loaded_image = Image.open(path_to_image).convert(\"RGB\")\n",
    "\n",
    "    # Bigger pictures will slow down the training process\n",
    "    MAXIMUM_SIZE = 500\n",
    "    if max(loaded_image.size) > MAXIMUM_SIZE:\n",
    "        size = (MAXIMUM_SIZE, MAXIMUM_SIZE)\n",
    "    else:\n",
    "        size = loaded_image.size\n",
    "\n",
    "    if shape is not None:\n",
    "        size = shape #.size gibt eh schon ein Tupel zurück\n",
    "\n",
    "    image_transformations = transforms.Compose([\n",
    "        transforms.Resize(size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.495, 0.455, 0.405),\n",
    "                             (0.255, 0.220, 0.230))\n",
    "    ])\n",
    "\n",
    "    loaded_image = image_transformations(loaded_image)[:3, :, :].unsqueeze(0)\n",
    "\n",
    "    return loaded_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398f2278",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1700749627389,
     "user": {
      "displayName": "Franziska Rubenbauer",
      "userId": "01843698082611866585"
     },
     "user_tz": -60
    },
    "id": "398f2278",
    "tags": []
   },
   "outputs": [],
   "source": [
    "original_image = load_image('uploads/contentimage.jpg').to(torch_device) #bilder zum torch.device schicken\n",
    "style_image = load_image(\"uploads/styleimage.jpg\", shape=original_image.shape[-2:]).to(torch_device) #Style Bild soll genau die gleiche Shape haben, wie das Orginalbild\n",
    "\n",
    "#visualize the image after tranformation\n",
    "def display_image(tensor_image):\n",
    "    image_to_show = tensor_image.to(\"cpu\").clone().detach() #Kopie erzeugen\n",
    "\n",
    "    image_to_show = image_to_show.numpy().squeeze()  #undo unsqueeze()\n",
    "    image_to_show = image_to_show.transpose(1, 2, 0)\n",
    "    image_to_show = image_to_show * numpy.array((0.255, 0.220, 0.230)) + numpy.array((0.495, 0.455, 0.405)) #unnormalize the image\n",
    "    image_to_show = image_to_show.clip(0, 1) #numpy.clip -> clips the edges\n",
    "\n",
    "    return image_to_show"
   ]
  },
  {
   "cell_type": "raw",
   "id": "760b7932-dc85-4b99-a27a-67f0f39c26b2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 802,
     "output_embedded_package_id": "1_kmY1u7wcOwYNUbmbdMl-gBpFWGXiLmb"
    },
    "execution": {
     "iopub.execute_input": "2024-03-01T11:06:10.437041Z",
     "iopub.status.busy": "2024-03-01T11:06:10.436077Z",
     "iopub.status.idle": "2024-03-01T11:06:10.874420Z",
     "shell.execute_reply": "2024-03-01T11:06:10.871752Z"
    },
    "executionInfo": {
     "elapsed": 16144,
     "status": "ok",
     "timestamp": 1700749643525,
     "user": {
      "displayName": "Franziska Rubenbauer",
      "userId": "01843698082611866585"
     },
     "user_tz": -60
    },
    "id": "a190b167",
    "outputId": "3e9b56ec-ef3d-400a-8c91-7abb820e35ce",
    "tags": []
   },
   "source": [
    "#figure, (axis1, axis2) = pyplot.subplots(1, 2, figsize=(30, 20))\n",
    "\n",
    "axis1 = pyplot.subplot(121)\n",
    "axis1.set_title(\"Inhaltsbild\")\n",
    "pyplot.axis('off')\n",
    "axis1.imshow(display_image(original_image))\n",
    "axis2 = pyplot.subplot(122)\n",
    "axis2.set_title(\"Stilbild \\n violin and jug \\n by Georges Braque\")\n",
    "axis2.imshow(display_image(style_image))\n",
    "pyplot.axis('off')\n",
    "pyplot.savefig(\"Eingabebilder.png\")\n",
    "#ensures that we didnt make a mistake by processing the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77faba87",
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1700749643527,
     "user": {
      "displayName": "Franziska Rubenbauer",
      "userId": "01843698082611866585"
     },
     "user_tz": -60
    },
    "id": "77faba87",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#extract the key features of the images\n",
    "def extract_features(image, model, layers = None):\n",
    "    if layers is None:\n",
    "        layers = {\n",
    "            \"0\": \"conv1_1\",\n",
    "            \"5\": \"conv2_1\",\n",
    "            \"10\": \"conv3_1\",\n",
    "            \"19\": \"conv4_1\",\n",
    "            \"25\": \"conv4_2\",\n",
    "            \"28\": \"conv5_1\"\n",
    "        }\n",
    "\n",
    "    input1 = image\n",
    "    features = {}\n",
    "\n",
    "    for name, layer in model._modules.items():\n",
    "        input1 = layer(input1)\n",
    "\n",
    "        if name in layers:\n",
    "            features[layers[name]] = input1\n",
    "\n",
    "    return features\n",
    "\n",
    "def calculate_gram_matrix(tensor_image):\n",
    "    batch_size, depth, height, width = tensor_image.size()\n",
    "\n",
    "    tensor_image = tensor_image.view(depth, -1)\n",
    "\n",
    "    gram_matrix = torch.mm(tensor_image, tensor_image.t()) #multiply by the transposed image\n",
    "\n",
    "    return gram_matrix\n",
    "\n",
    "def compute_total_variation_loss(Y_hat):\n",
    "    return 0.5 * (torch.abs(Y_hat[:, :, 1:, :] - Y_hat[:, :, :-1, :]).mean() +\n",
    "                  torch.abs(Y_hat[:, :, :, 1:] - Y_hat[:, :, :, :-1]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28963b24",
   "metadata": {
    "executionInfo": {
     "elapsed": 18610,
     "status": "ok",
     "timestamp": 1700749662119,
     "user": {
      "displayName": "Franziska Rubenbauer",
      "userId": "01843698082611866585"
     },
     "user_tz": -60
    },
    "id": "28963b24",
    "tags": []
   },
   "outputs": [],
   "source": [
    "original_features = extract_features(original_image, vgg_model)\n",
    "style_features = extract_features(style_image, vgg_model)\n",
    "\n",
    "style_gram_matrices = {layer: calculate_gram_matrix(style_features[layer]) for layer in style_features}#calculate gram_matrix for every layer\n",
    "\n",
    "altered_image = original_image.clone().requires_grad_(True).to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaf330e",
   "metadata": {
    "executionInfo": {
     "elapsed": 103,
     "status": "ok",
     "timestamp": 1700749662121,
     "user": {
      "displayName": "Franziska Rubenbauer",
      "userId": "01843698082611866585"
     },
     "user_tz": -60
    },
    "id": "4eaf330e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "style_weights = {\"conv1_1\": 1.,\n",
    "                 \"conv2_1\": 0.7,\n",
    "                 \"conv3_1\": 0.4,\n",
    "                 \"conv4_1\": 0.2,\n",
    "                 \"conv5_1\": 0.1}\n",
    "\n",
    "optimizer = optimizers.Adam([altered_image])\n",
    "original_image_weight = 1\n",
    "style_image_weight = 10000\n",
    "total_variation_weight = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3205bd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "bc3205bd",
    "outputId": "20215b01-18c4-450e-93ce-67d0b6f606f1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUMBER_OF_EPOCHS = 10000 #can change this\n",
    "\n",
    "total_loss_for_plt = []  # Initialize an empty list to store total losses\n",
    "style_loss_for_plt = []  # Initialize an empty list to store style losses\n",
    "content_loss_for_plt = []  # Initialize an empty list to store content losses\n",
    "\n",
    "for index in range(1, NUMBER_OF_EPOCHS+1):\n",
    "    altered_features = extract_features(altered_image, vgg_model) #extract features at every epoch\n",
    "\n",
    "    style_loss = 0\n",
    "\n",
    "    original_image_loss = torch.mean((altered_features[\"conv4_2\"] - original_features[\"conv4_2\"]) **2) #mean squared error\n",
    "\n",
    "    for layer in style_weights:\n",
    "        altered_feature = altered_features[layer] #get the feature for every single weight\n",
    "\n",
    "        altered_gram_matrix = calculate_gram_matrix(altered_feature)\n",
    "\n",
    "        style_matrix = style_gram_matrices[layer]\n",
    "\n",
    "        layer_style_loss = torch.mean((altered_gram_matrix - style_matrix) ** 2) * style_weights[layer] #mean squared error * layer_weight\n",
    "\n",
    "        _, depth, height, width = altered_feature.shape\n",
    "\n",
    "        style_loss += layer_style_loss / (depth * height * width)\n",
    "        \n",
    "        total_variation_loss = compute_total_variation_loss(altered_feature) * total_variation_weight\n",
    "\n",
    "    total_loss = original_image_loss * original_image_weight + style_loss * style_image_weight\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    total_loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    if index % 1000 == 0:\n",
    "        pyplot.imshow(display_image(altered_image))\n",
    "        if index != NUMBER_OF_EPOCHS:\n",
    "            pyplot.title(\"Epoche: \" + str(index))\n",
    "        pyplot.axis('off')\n",
    "        pyplot.savefig(\"Epochenbild.png\")\n",
    "        if index == NUMBER_OF_EPOCHS:\n",
    "            pyplot.savefig(\"Ausgabebild.png\")\n",
    "        pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cbe251-0a5c-4b19-a38f-27100f8a3f6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0957fea-e84f-4777-90a7-ece436a538e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4eb8684-dd0a-42a8-9ca8-26c691b39f95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7271582f-c0a1-472c-93ee-e0e7c28e41f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
